# ðŸ›Ÿ Titanic Survival â€” **AWS SageMaker MLOps** Project (Pipelines + GitHub Actions) 

![AWS](https://img.shields.io/badge/AWS-SageMaker-FF9900?logo=amazon-aws&logoColor=white)
![MLOps](https://img.shields.io/badge/MLOps-Pipeline-2088FF)

End-to-end MLOps on **AWS SageMaker** using the Titanic dataset:  
**data â†’ preprocessing â†’ training â†’ evaluation â†’ registry â†’ deployment (endpoint) â†’ monitoring â†’ CI/CD**.

âœ… Data prep to S3
âœ… SageMaker Pipeline (Preprocess â†’ Train â†’ Evaluate â†’ Gate â†’ Register)
âœ… Model Registry (Pending/Manual approval by default)
âœ… Smoke test against a real endpoint
âœ… Monitoring-ready (data capture + Model Monitor helpers)
âœ… OIDC for GitHub Actions (no long-lived keys)

---

## ðŸ“Š Problem Statement
Predict **survival** based on:

- **Pclass**, **Sex**, **Age**, **SibSp**, **Parch**, **Fare**, **Embarked**
- Model: **XGBoost (built-in, 1.7-1)**

---

## ðŸ§± Repo Structure

sagemaker-titanic-mlops/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ titanic.csv                     # sample local dataset (uploaded to S3)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py                   # writes train/test (label first col, no header)
â”‚   â”œâ”€â”€ evaluate.py                     # writes metrics.json { auc, accuracy }
â”‚   â”œâ”€â”€ enable_data_capture.py          # optional: turn on endpoint capture
â”‚   â”œâ”€â”€ monitor_setup.py                # optional: Model Monitor schedule
â”‚   â””â”€â”€ register_model.py               # optional: extra registry helpers
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ pipeline_up.py                  # defines & triggers SageMaker Pipeline
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ mlops.yml                       # smoke + pipeline jobs (OIDC supported)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile                         
â””â”€â”€ README.md


## ðŸš€ Quick Start

### âœ… Phase 1 â€” Project Initialization
- Local env & repo structure (`src/`, `pipelines/`, `.github/workflows/`)
- `requirements.txt`, `.gitignore`
- AWS access configured (IAM execution role for SageMaker jobs)

**Quickstart**

git clone https://github.com/Meerasachp/sagemaker-titanic-mlops.git
cd sagemaker-titanic-mlops
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt
aws configure   # use us-east-1

### âœ… Phase 2 â€” Data Ingestion (feature-engineering ready)

Titanic CSV stored in your S3 bucket.
Preprocess expects label in first column; writes train.csv & test.csv.

Create a bucket you own (unique) and upload the CSV
ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
BUCKET="sagemaker-us-east-1-${ACCOUNT}-mlops"
aws s3api create-bucket --bucket "$BUCKET"
aws s3 cp ./data/titanic.csv "s3://${BUCKET}/raw/titanic.csv"
aws s3 ls "s3://${BUCKET}/raw/"
â„¹ï¸ Feature Store is optional for v1.0; preprocess is feature-store compatible if you add it later.

### âœ… Phase 3 â€” Training & Evaluation (XGBoost)

- Training uses SageMaker built-in XGBoost (1.7-1).
- Evaluation produces metrics.json with auc & accuracy.

core scripts
src/preprocess.py â†’ reads CSV from input dir; writes:
/opt/ml/processing/train/train.csv
/opt/ml/processing/test/test.csv
src/evaluate.py â†’ loads model.tar.gz, computes metrics â†’ /opt/ml/processing/output/metrics.json

### âœ… Phase 4 â€” Model Registry & Deployment

- Registered models land in Model Package Group (e.g., titanic-xgboost) with PendingManualApproval.
- A live endpoint is used by the CI Smoke test (boto3 invoke).

### âœ… Phase 5 â€” Monitoring

- Data capture & Model Monitor schedule (hourly) supported via helpers:
  src/enable_data_capture.py
  src/monitor_setup.py
  src/register_model.py
 Cost guardrails: sampling â‰¤ 25% and S3 lifecycle (30d) for monitoring/ & datacapture/.

### âœ… Phase 6 â€” CI/CD Automation (GitHub Actions + SageMaker Pipelines)

Smoke: invokes endpoint on push/PR.
SageMaker Pipeline: Preprocess â†’ Train â†’ Evaluate â†’ Quality Gate (AUC) â†’ Register.
Gate defaults to AUC â‰¥ 0.80 (override via env).
Instance types (safe defaults, override via env):
   Processing/Eval: ml.t3.medium
   Training (XGBoost allow-list): ml.m4.xlarge
Pipeline file: pipelines/pipeline_up.py (defines & starts the run).

### ðŸ§° Tech Stack

AWS SageMaker: Processing, Training (XGBoost), Pipelines, Model Registry, Endpoints
CI/CD: GitHub Actions (smoke + pipeline jobs; OIDC supported)
Storage & Logs: Amazon S3 (artifacts, capture, metrics), CloudWatch Logs
Libraries: XGBoost, pandas, scikit-learn, boto3, SageMaker SDK
Security: IAM execution role, OIDC role assumption for CI (no static keys), fork-PR secret gating



### â–¶ï¸ How to Run

A) End-to-end via CI

Push to main or open Actions â†’ â€œMLOps Pipeline (Smoke)â€ â†’ Run workflow
Watch smoke logs (prints JSON prediction)
Watch sagemaker-pipeline logs (upserts & starts TitanicXGBPipeline)
In Studio â†’ Pipelines, open latest execution to see steps & metrics

B) Quick CLI checks

##Latest pipeline execution status
aws sagemaker list-pipeline-executions \
  --pipeline-name TitanicXGBPipeline --max-results 1 \
  --query "PipelineExecutionSummaries[0].[PipelineExecutionArn,PipelineExecutionStatus]"

##Latest registered model package 
aws sagemaker list-model-packages \
  --model-package-group-name titanic-xgboost \
  --query "ModelPackageSummaryList[0].[ModelPackageArn,ModelApprovalStatus]"

### ðŸ§ª Example Prediction (from Smoke)

  {"predictions": [{"score": 0.8977}]}

# ðŸ§© Known Gotchas / Troubleshooting

--> ValidationError: Endpoint <name> of account <acct> not found 
--> Check region (us-east-1 vs others) and AWS profile.
--> Confirm endpoint status is InService.
--> Double-check the endpoint name (typos, suffixes).
--> Invalid endpoint URL (double dot) like runtime.sagemaker..amazonaws.com
--> Usually a malformed region var. Ensure AWS_REGION is set (e.g., us-east-1).
--> In boto3, pass region_name explicitly in clients used by CI.
--> S3 bucket errors
--> Bucket names must be globally unique and match region.
--> Use the --create-bucket-configuration only when region â‰  us-east-1.
--> GitHub Actions credentials
--> Prefer OIDC role. If using secrets, set AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION and verify aws sts
    get-caller-identity.


## Meerasa â€” DevOps / MLOps Engineer :-)





