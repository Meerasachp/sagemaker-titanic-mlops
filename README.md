# ğŸ›Ÿ Titanic Survival â€” **AWS SageMaker MLOps** Project (Pipelines + GitHub Actions) 

![AWS](https://img.shields.io/badge/AWS-SageMaker-FF9900?logo=amazon-aws&logoColor=white)
![MLOps](https://img.shields.io/badge/MLOps-Pipeline-2088FF)

End-to-end MLOps on **AWS SageMaker** using the Titanic dataset:  
**data â†’ preprocessing â†’ training â†’ evaluation â†’ registry â†’ deployment (endpoint) â†’ monitoring â†’ CI/CD**.

âœ… Data prep to S3  
âœ… SageMaker Pipeline (Preprocess â†’ Train â†’ Evaluate â†’ Gate â†’ Register)  
âœ… Model Registry (Pending/Manual approval by default)  
âœ… Smoke test against a real endpoint  
âœ… Monitoring-ready (data capture + Model Monitor helpers)  
âœ… OIDC for GitHub Actions (no long-lived keys)  

---

## ğŸ“Š Problem Statement
Predict **survival** based on:

- **Pclass**, **Sex**, **Age**, **SibSp**, **Parch**, **Fare**, **Embarked**
- Model: **XGBoost (built-in, 1.7-1)**

---

## ğŸ§± Repo Structure

```bash
sagemaker-titanic-mlops/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ titanic.csv                     # sample local dataset (uploaded to S3)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocess.py                   # writes train/test (label first col, no header)
â”‚   â”œâ”€â”€ evaluate.py                     # writes metrics.json { auc, accuracy }
â”‚   â”œâ”€â”€ enable_data_capture.py          # optional: turn on endpoint capture
â”‚   â”œâ”€â”€ monitor_setup.py                # optional: Model Monitor schedule
â”‚   â””â”€â”€ register_model.py               # optional: extra registry helpers
â”œâ”€â”€ pipelines/
â”‚   â””â”€â”€ pipeline_up.py                  # defines & triggers SageMaker Pipeline
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ mlops.yml                       # smoke + pipeline jobs (OIDC supported)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Makefile                         
â””â”€â”€ README.md

â–¶ï¸ To Start the Project 

git clone https://github.com/Meerasachp/sagemaker-titanic-mlops.git
cd sagemaker-titanic-mlops
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt
aws configure   # use us-east-1

ğŸš€ Quick Start

âœ…Phase 1 â€” Project Initialization
Local env & repo structure (src/, pipelines/, .github/workflows/)
requirements.txt, .gitignore
AWS access configured (IAM execution role for SageMaker jobs)

âœ… Phase 2 â€” Data Ingestion (feature-engineering ready)
Titanic CSV stored in your S3 bucket. Preprocess expects label in first column; writes train.csv & test.csv.

ACCOUNT=$(aws sts get-caller-identity --query Account --output text)
BUCKET="sagemaker-us-east-1-${ACCOUNT}-mlops"
aws s3api create-bucket --bucket "$BUCKET"
aws s3 cp ./data/titanic.csv "s3://${BUCKET}/raw/titanic.csv"
aws s3 ls "s3://${BUCKET}/raw/"

âœ… Phase 4 â€” Model Registry & Deployment
Data capture & Model Monitor (hourly) via helpers:
src/enable_data_capture.py, src/monitor_setup.py, src/register_model.py
Cost guardrails: sampling â‰¤ 25% + S3 lifecycle (30d).

âœ… Phase 6 â€” CI/CD Automation (GitHub Actions + SageMaker Pipelines)
moke test: invokes endpoint on push/PR.
Pipeline: Preprocess â†’ Train â†’ Evaluate â†’ Gate (AUC â‰¥ 0.80) â†’ Register.
Instance defaults:
   Processing/Eval â†’ ml.t3.medium
   Training â†’ ml.m4.xlarge

## ğŸ§° Tech Stack:
AWS SageMaker: Processing, Training, Pipelines, Model Registry, Endpoints
CI/CD: GitHub Actions (smoke + pipeline jobs; OIDC supported)
Storage/Logs: Amazon S3, CloudWatch Logs
Libraries: XGBoost, pandas, scikit-learn, boto3, SageMaker SDK
Security: IAM execution role, OIDC role assumption (no static keys)

â–¶ï¸ How to Run

A) End-to-end via CI
Push to main or open Actions â†’ â€œMLOps Pipeline (Smoke)â€ â†’ Run workflow
Watch smoke logs (prints JSON prediction)
View pipeline execution in SageMaker Studio

B) Quick CLI checks
# Latest pipeline execution status
aws sagemaker list-pipeline-executions \
  --pipeline-name TitanicXGBPipeline --max-results 1 \
  --query "PipelineExecutionSummaries[0].[PipelineExecutionArn,PipelineExecutionStatus]"

# Latest registered model package 
aws sagemaker list-model-packages \
  --model-package-group-name titanic-xgboost \
  --query "ModelPackageSummaryList[0].[ModelPackageArn,ModelApprovalStatus]"

## ğŸ§ª Example Prediction (from Smoke)

{"predictions": [{"score": 0.8977}]}


# ğŸ§© Known Gotchas / Troubleshooting
ValidationError: Endpoint not found
â†’ Check region/profile; confirm endpoint is InService; no typos; region var set.

Invalid endpoint URL (double dot)
â†’ Ensure AWS_REGION is correct.

S3 bucket errors
â†’ Bucket names must be unique + region-matching.
â†’ --create-bucket-configuration only needed outside us-east-1.

GitHub Actions credentials
â†’ Prefer OIDC role. If secrets used, set:
AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, AWS_REGION


# Meerasa â€” DevOps / MLOps Engineer :-)

